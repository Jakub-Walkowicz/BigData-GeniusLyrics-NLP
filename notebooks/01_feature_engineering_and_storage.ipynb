{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6d7265ab8b1870",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:29:47.161523Z",
     "start_time": "2025-12-26T10:29:47.101277Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324dd685b21bb4fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T09:53:48.865322Z",
     "start_time": "2025-12-27T09:53:48.743940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d749102f-b484-4096-be97-5180a8145bcd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (264ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (139ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (384ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (132ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (96ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (83ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (159ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (89ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (147ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (91ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (88ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (105ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (92ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (83ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (85ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (83ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (97ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (101ms)\n",
      ":: resolution report :: resolve 9263ms :: artifacts dl 2329ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d749102f-b484-4096-be97-5180a8145bcd\n",
      "\tconfs: [default]\n",
      "\t18 artifacts copied, 0 already retrieved (18067kB/30ms)\n",
      "26/01/01 21:52:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/opt/spark/python/pyspark/context.py:350: RuntimeWarning: Failed to add file [/opt/spark/work-dir/src] specified in 'spark.submit.pyFiles' to Python path:\n",
      "  /opt/spark/python\n",
      "  /tmp/spark-b250850d-66bd-4fb6-ba29-646661f6d19b/userFiles-3144e30e-5831-49fc-aacb-7226f0d57868\n",
      "  /opt/spark/python/lib/py4j-0.10.9.7-src.zip\n",
      "  /opt/spark/work-dir/src\n",
      "  /usr/lib/python310.zip\n",
      "  /usr/lib/python3.10\n",
      "  /usr/lib/python3.10/lib-dynload\n",
      "  \n",
      "  /usr/local/lib/python3.10/dist-packages\n",
      "  /usr/lib/python3/dist-packages\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from spark.spark_config import create_spark_session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:29:51.228265Z",
     "start_time": "2025-12-26T10:29:50.692361Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_loader import DataLoader\n",
    "from preprocessor import Preprocessor\n",
    "from nlp_processor import NLPProcessor\n",
    "from feature_extractor import FeatureExtractor\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "57d965273198c963"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef726546266f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:29:52.275838Z",
     "start_time": "2025-12-26T10:29:51.232761Z"
    }
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(spark)\n",
    "df = data_loader.load_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23e12d14bf27c2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:29:52.302776Z",
     "start_time": "2025-12-26T10:29:52.289732Z"
    }
   },
   "outputs": [],
   "source": [
    "# analyser = EdaAnalyser(df)\n",
    "# analyser.run_full_eda_report(['title', 'lyrics', 'views'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf0162e9e80b439",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:29:52.402530Z",
     "start_time": "2025-12-26T10:29:52.303877Z"
    }
   },
   "outputs": [],
   "source": [
    "df = Preprocessor.run(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "903d8c3655832b2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:29:52.889201Z",
     "start_time": "2025-12-26T10:29:52.407552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[title: string, tag: string, artist: string, year: int, views: int, features: string, lyrics: string, id: int, language_cld3: string, language_ft: string, language: string, lyrics_cleaned: string, words_lemmatized: array<string>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokenized = NLPProcessor.run(df)\n",
    "df_tokenized.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de289ba8c53f5e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:33:11.717840Z",
     "start_time": "2025-12-26T10:29:52.917781Z"
    }
   },
   "outputs": [],
   "source": [
    "extractor = FeatureExtractor()\n",
    "# extractor.fit(df_tokenized)\n",
    "# df_final = extractor.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e38c4c8da7f86c4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:36:12.863392Z",
     "start_time": "2025-12-26T10:36:12.367688Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "base_dir = Path.cwd().parent\n",
    "test_file_path = base_dir / \"models\" / \"test\" / \"word2vec\"\n",
    "# extractor.save(str(test_file_path))\n",
    "df_final = extractor.load(str(test_file_path)).transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83c9f97-81b7-45d6-a749-6877207d80d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `feature_array` cannot be resolved. Did you mean one of the following? [`features`, `year`, `artist`, `language`, `language_cld3`].;\n'Project ['feature_array]\n+- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, lyrics_cleaned#34, words_lemmatized#89, UDF(words_lemmatized#89) AS word_vectors#196]\n   +- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, lyrics_cleaned#34, words_lemmatized#89]\n      +- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, lyrics_cleaned#34, words_tokens#50, words_filtered#70, lemmatize_udf(words_filtered#70)#88 AS words_lemmatized#89]\n         +- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, lyrics_cleaned#34, words_tokens#50, UDF(words_tokens#50) AS words_filtered#70]\n            +- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, lyrics_cleaned#34, UDF(lyrics_cleaned#34) AS words_tokens#50]\n               +- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, trim(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(lyrics#6, \\[[^\\]]*\\], , 1), \\([^)]*\\), , 1), [‘’`´], ', 1), [ \\t]+,  , 1),  *\\n *, \n, 1), ^\\s+, , 1), \\s+$, , 1), None) AS lyrics_cleaned#34]\n                  +- Filter (language#10 = pl)\n                     +- Filter atleastnnonnulls(1, lyrics#6)\n                        +- Relation [title#0,tag#1,artist#2,year#3,views#4,features#5,lyrics#6,id#7,language_cld3#8,language_ft#9,language#10] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_final\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfeature_array\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3036\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2991\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2992\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2993\u001B[0m \n\u001B[1;32m   2994\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3034\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3036\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3037\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
      "File \u001B[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    171\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 175\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `feature_array` cannot be resolved. Did you mean one of the following? [`features`, `year`, `artist`, `language`, `language_cld3`].;\n'Project ['feature_array]\n+- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, lyrics_cleaned#34, words_lemmatized#89, UDF(words_lemmatized#89) AS word_vectors#196]\n   +- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, lyrics_cleaned#34, words_lemmatized#89]\n      +- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, lyrics_cleaned#34, words_tokens#50, words_filtered#70, lemmatize_udf(words_filtered#70)#88 AS words_lemmatized#89]\n         +- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, lyrics_cleaned#34, words_tokens#50, UDF(words_tokens#50) AS words_filtered#70]\n            +- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, lyrics_cleaned#34, UDF(lyrics_cleaned#34) AS words_tokens#50]\n               +- Project [title#0, tag#1, artist#2, year#3, views#4, features#5, lyrics#6, id#7, language_cld3#8, language_ft#9, language#10, trim(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(lyrics#6, \\[[^\\]]*\\], , 1), \\([^)]*\\), , 1), [‘’`´], ', 1), [ \\t]+,  , 1),  *\\n *, \n, 1), ^\\s+, , 1), \\s+$, , 1), None) AS lyrics_cleaned#34]\n                  +- Filter (language#10 = pl)\n                     +- Filter atleastnnonnulls(1, lyrics#6)\n                        +- Relation [title#0,tag#1,artist#2,year#3,views#4,features#5,lyrics#6,id#7,language_cld3#8,language_ft#9,language#10] parquet\n"
     ]
    }
   ],
   "source": [
    "df_final.select(\"feature_array\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a93c4c88-ffda-4519-bafd-b2ed77b59b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from repository.cassandra_provider import CassandraProvider \n",
    "cassandra_provider = CassandraProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23e2727b-83aa-43b2-9e8e-d7de44ba39fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/01 21:54:02 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 9) (172.20.0.5 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 814, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'nlp_processor'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "26/01/01 21:54:05 ERROR TaskSetManager: Task 0 in stage 7.0 failed 4 times; aborting job\n",
      "26/01/01 21:54:05 ERROR AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@637751ec,com.datastax.spark.connector.cql.CassandraConnector@5647bd5d,TableDef(genius_space,processed_songs,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,BigIntType)),ArrayBuffer(),Stream(ColumnDef(artist,RegularColumn,VarCharType), ColumnDef(feature_array,RegularColumn,ListType(DoubleType,false)), ColumnDef(tag,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(views,RegularColumn,BigIntType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,IntegerType,true),StructField(artist,StringType,true),StructField(title,StringType,true),StructField(views,IntegerType,true),StructField(tag,StringType,true),StructField(feature_array,ArrayType(DoubleType,false),false)),org.apache.spark.SparkConf@25a5fcad) is aborting.\n",
      "26/01/01 21:54:05 ERROR AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@637751ec,com.datastax.spark.connector.cql.CassandraConnector@5647bd5d,TableDef(genius_space,processed_songs,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,BigIntType)),ArrayBuffer(),Stream(ColumnDef(artist,RegularColumn,VarCharType), ColumnDef(feature_array,RegularColumn,ListType(DoubleType,false)), ColumnDef(tag,RegularColumn,VarCharType), ColumnDef(title,RegularColumn,VarCharType), ColumnDef(views,RegularColumn,BigIntType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,IntegerType,true),StructField(artist,StringType,true),StructField(title,StringType,true),StructField(views,IntegerType,true),StructField(tag,StringType,true),StructField(feature_array,ArrayType(DoubleType,false),false)),org.apache.spark.SparkConf@25a5fcad) aborted.\n",
      "26/01/01 21:54:05 WARN TaskSetManager: Lost task 3.1 in stage 7.0 (TID 18) (172.20.0.5 executor 0): TaskKilled (Stage cancelled)\n",
      "26/01/01 21:54:05 WARN TaskSetManager: Lost task 1.3 in stage 7.0 (TID 21) (172.20.0.5 executor 0): TaskKilled (Stage cancelled)\n",
      "26/01/01 21:54:06 WARN TaskSetManager: Lost task 2.2 in stage 7.0 (TID 20) (172.20.0.5 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 814, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'nlp_processor'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPythonException\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcassandra_provider\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_final\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/spark/work-dir/src/repository/cassandra_provider.py:18\u001B[0m, in \u001B[0;36mCassandraProvider.save\u001B[0;34m(self, df)\u001B[0m\n\u001B[1;32m      9\u001B[0m required_cols \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124martist\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mviews\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtag\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature_array\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     11\u001B[0m df_to_save \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature_array\u001B[39m\u001B[38;5;124m\"\u001B[39m, vector_to_array(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword_vectors\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     13\u001B[0m (\u001B[43mdf_to_save\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrequired_cols\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43morg.apache.spark.sql.cassandra\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mappend\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeyspace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeyspace\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m---> 18\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1396\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1394\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m   1395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1396\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1397\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1398\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
      "File \u001B[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    171\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 175\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mPythonException\u001B[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 814, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'nlp_processor'\n"
     ]
    }
   ],
   "source": [
    "cassandra_provider.save(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a1c78-445a-4e12-a44b-d178f9fd917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Podsumowanie poprawek\n",
    "Zmień bind mount na named volume (cassandra_data), aby uniknąć błędów \"Permission denied\".\n",
    "\n",
    "Dodaj zmienne środowiskowe CASSANDRA_USER i CASSANDRA_PASSWORD.\n",
    "\n",
    "Ustaw CASSANDRA_AUTHENTICATOR na PasswordAuthenticator, bo domyślnie Cassandra pozwala na wejście każdemu bez hasła (AllowAllAuthenticator)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
