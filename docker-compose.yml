services:
  cassandra:
    container_name: cassandra
    hostname: cassandra
    image: cassandra:4.1
    ports:
      - '9042:9042'
    env_file:
      - docker/cassandra/cassandra.env
    volumes:
      - cassandra_data:/var/lib/cassandra
    healthcheck:
      test: [ "CMD", "nodetool", "status" ]
      interval: 20s
      timeout: 5s
      retries: 5
    restart: on-failure
    deploy:
      resources:
        limits:
          memory: 4G

  cassandra-init:
    image: cassandra:4.1
    container_name: cassandra-init
    depends_on:
      cassandra:
        condition: service_healthy
    volumes:
      - ./infrastructure/migration/init.cql:/init.cql    
    command: /bin/bash -c "cqlsh cassandra -f /init.cql"

  spark-master:
    container_name: spark-master
    hostname: spark-master
    build:
      context: .
      dockerfile: ./docker/spark/Dockerfile
    env_file:
      - docker/spark/spark-master.env
    ports:
      - '8080:8080'
      - '7077:7077'
    volumes:
      - ./data:/opt/spark/work-dir/data:ro
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    deploy:
      resources:
        limits:
          memory: 2G

  spark-worker:
    container_name: spark-worker
    build:
      context: .
      dockerfile: ./docker/spark/Dockerfile
    env_file:
      - docker/spark/spark-worker.env
    ports:
      - '8081:8081'
    depends_on:
      spark-master:
        condition: service_healthy
      cassandra:
        condition: service_healthy
    volumes:
      - ./data:/opt/spark/work-dir/data:ro
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    deploy:
      resources:
        limits:
          memory: 4G

  jupyter:
    container_name: jupyter
    build:
      context: .
      dockerfile: ./docker/spark/Dockerfile
    ports:
      - '8888:8888'
    env_file:
      - docker/spark/jupyter.env
    volumes:
      - ./data:/opt/spark/work-dir/data:ro
    depends_on:
      spark-master:
        condition: service_healthy
    command: jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --NotebookApp.token=''
    deploy:
      resources:
        limits:
          memory: 2G
volumes:
  cassandra_data:
